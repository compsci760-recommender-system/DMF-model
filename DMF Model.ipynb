{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import heapq\n",
    "import math\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code from https://github.com/RuidongZ/Deep_Matrix_Factorization_Models: Dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "    def __init__(self, fileName):\n",
    "        self.data, self.shape = self.getData(fileName)\n",
    "        self.train, self.test = self.getTrainTest()\n",
    "        self.trainDict = self.getTrainDict()\n",
    "\n",
    "    def getData(self, fileName):\n",
    "        if fileName == 'ml-1m':\n",
    "            print(\"Loading ml-1m data set...\")\n",
    "            data = []\n",
    "            filePath = './data/ml-1m/ratings.dat'\n",
    "            highest_user_id = 0\n",
    "            highest_item_id = 0\n",
    "            maxr = 0.0\n",
    "            with open(filePath, 'r') as f:\n",
    "                for line in f:\n",
    "                    if line:\n",
    "                        lines = line[:-1].split(\"::\")\n",
    "                        user = int(lines[0])\n",
    "                        movie = int(lines[1])\n",
    "                        score = float(lines[2])\n",
    "                        time = int(lines[3])\n",
    "                        data.append((user, movie, score, time))\n",
    "                        if user > highest_user_id:\n",
    "                            highest_user_id = user\n",
    "                        if movie > highest_item_id:\n",
    "                            highest_item_id = movie\n",
    "                        if score > maxr:\n",
    "                            maxr = score\n",
    "            self.maxRate = maxr\n",
    "            print(\"Loading Success!\\n\"\n",
    "                  \"Data Info:\\n\"\n",
    "                  \"\\tUser Num: {}\\n\"\n",
    "                  \"\\tItem Num: {}\\n\"\n",
    "                  \"\\tData Size: {}\".format(highest_user_id, highest_item_id, len(data)))\n",
    "            return data, [highest_user_id, highest_item_id]\n",
    "        else:\n",
    "            print(\"Current data set is not support!\")\n",
    "            sys.exit()\n",
    "\n",
    "    def getTrainTest(self):\n",
    "        \"\"\"Only the last rating for each user is picked for the test set. The user_id and the item_id is decremented by one. Timestamp is removed.\"\"\"\n",
    "        data = self.data\n",
    "        data = sorted(data, key=lambda x: (x[0], x[3]))\n",
    "        train = []\n",
    "        test = []\n",
    "        for i in range(len(data)-1):\n",
    "            user = data[i][0]-1\n",
    "            item = data[i][1]-1\n",
    "            rate = data[i][2]\n",
    "            if data[i][0] != data[i+1][0]:\n",
    "                test.append((user, item, rate))\n",
    "            else:\n",
    "                train.append((user, item, rate))\n",
    "        test.append((data[-1][0]-1, data[-1][1]-1, data[-1][2]))\n",
    "        return train, test\n",
    "\n",
    "    def getTrainDict(self):\n",
    "        \"\"\"Creates a dictionary with a tuple as a key and the rating as the value. The tuple is a user-item pair.\"\"\"\n",
    "        dataDict = {}\n",
    "        for i in self.train:\n",
    "            dataDict[(i[0], i[1])] = i[2]\n",
    "        return dataDict\n",
    "\n",
    "    def getEmbedding(self):\n",
    "        \"\"\"Creates a matrix of all the user-item ratings from the train set.\"\"\"\n",
    "        train_matrix = np.zeros([self.shape[0], self.shape[1]], dtype=np.float32)\n",
    "        for i in self.train:\n",
    "            user = i[0]\n",
    "            movie = i[1]\n",
    "            rating = i[2]\n",
    "            train_matrix[user][movie] = rating\n",
    "        return np.array(train_matrix)\n",
    "\n",
    "    def getInstances(self, data, negNum):\n",
    "        \"\"\"Creates lists of users, items and rates. For each rating from the dataset there's added negNum 0.0 ratings.\"\"\"\n",
    "        user = []\n",
    "        item = []\n",
    "        rate = []\n",
    "        for i in data:\n",
    "            user.append(i[0])\n",
    "            item.append(i[1])\n",
    "            rate.append(i[2])\n",
    "            for t in range(negNum):\n",
    "                j = np.random.randint(self.shape[1])\n",
    "                while (i[0], j) in self.trainDict:\n",
    "                    j = np.random.randint(self.shape[1])\n",
    "                user.append(i[0])\n",
    "                item.append(j)\n",
    "                rate.append(0.0)\n",
    "        return np.array(user), np.array(item), np.array(rate)\n",
    "\n",
    "    def getTestNeg(self, testData, negNum):\n",
    "        \"\"\"For each user, finds negNum items that does not make up a user-item pair in the train set.\"\"\"\n",
    "        user = []\n",
    "        item = []\n",
    "        for s in testData:\n",
    "            tmp_user = []\n",
    "            tmp_item = []\n",
    "            u = s[0]\n",
    "            i = s[1]\n",
    "            tmp_user.append(u)\n",
    "            tmp_item.append(i)\n",
    "            neglist = set()\n",
    "            neglist.add(i)\n",
    "            for t in range(negNum):\n",
    "                # Picks a random number not higher than the highest item id\n",
    "                j = np.random.randint(self.shape[1])\n",
    "                while (u, j) in self.trainDict or j in neglist:\n",
    "                    j = np.random.randint(self.shape[1])\n",
    "                neglist.add(j)\n",
    "                tmp_user.append(u)\n",
    "                tmp_item.append(j)\n",
    "            user.append(tmp_user)\n",
    "            item.append(tmp_item)\n",
    "        return [np.array(user), np.array(item)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code from https://github.com/RuidongZ/Deep_Matrix_Factorization_Models: Model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Options\")\n",
    "\n",
    "    parser.add_argument('-dataName', action='store', dest='dataName', default='ml-1m')\n",
    "    parser.add_argument('-negNum', action='store', dest='negNum', default=7, type=int)\n",
    "    parser.add_argument('-userLayer', action='store', dest='userLayer', default=[512, 64])\n",
    "    parser.add_argument('-itemLayer', action='store', dest='itemLayer', default=[1024, 64])\n",
    "    # parser.add_argument('-reg', action='store', dest='reg', default=1e-3)\n",
    "    parser.add_argument('-lr', action='store', dest='lr', default=0.0001)\n",
    "    parser.add_argument('-maxEpochs', action='store', dest='maxEpochs', default=50, type=int)\n",
    "    parser.add_argument('-batchSize', action='store', dest='batchSize', default=256, type=int)\n",
    "    parser.add_argument('-earlyStop', action='store', dest='earlyStop', default=5)\n",
    "    parser.add_argument('-checkPoint', action='store', dest='checkPoint', default='./checkPoint/')\n",
    "    parser.add_argument('-topK', action='store', dest='topK', default=10)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    classifier = Model(args)\n",
    "\n",
    "    classifier.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, args):\n",
    "        self.dataName = args.dataName\n",
    "        self.dataSet = DataSet(self.dataName)\n",
    "        self.shape = self.dataSet.shape\n",
    "        self.maxRate = self.dataSet.maxRate\n",
    "\n",
    "        self.train = self.dataSet.train\n",
    "        self.test = self.dataSet.test\n",
    "\n",
    "        self.negNum = args.negNum\n",
    "        self.testNeg = self.dataSet.getTestNeg(self.test, 99)\n",
    "        self.add_embedding_matrix()\n",
    "\n",
    "        self.add_placeholders()\n",
    "\n",
    "        self.userLayer = args.userLayer\n",
    "        self.itemLayer = args.itemLayer\n",
    "        self.add_model()\n",
    "\n",
    "        self.add_loss()\n",
    "\n",
    "        self.lr = args.lr\n",
    "        self.add_train_step()\n",
    "\n",
    "        self.checkPoint = args.checkPoint\n",
    "        self.init_sess()\n",
    "\n",
    "        self.maxEpochs = args.maxEpochs\n",
    "        self.batchSize = args.batchSize\n",
    "\n",
    "        self.topK = args.topK\n",
    "        self.earlyStop = args.earlyStop\n",
    "\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        self.user = tf.placeholder(tf.int32)\n",
    "        self.item = tf.placeholder(tf.int32)\n",
    "        self.rate = tf.placeholder(tf.float32)\n",
    "        self.drop = tf.placeholder(tf.float32)\n",
    "\n",
    "    def add_embedding_matrix(self):\n",
    "        self.user_item_embedding = tf.convert_to_tensor(self.dataSet.getEmbedding())\n",
    "        self.item_user_embedding = tf.transpose(self.user_item_embedding)\n",
    "\n",
    "    def add_model(self):\n",
    "        user_input = tf.nn.embedding_lookup(self.user_item_embedding, self.user)\n",
    "        item_input = tf.nn.embedding_lookup(self.item_user_embedding, self.item)\n",
    "\n",
    "        def init_variable(shape, name):\n",
    "            return tf.Variable(tf.truncated_normal(shape=shape, dtype=tf.float32, stddev=0.01), name=name)\n",
    "\n",
    "        \"\"\"Builds the neural network.\"\"\"\n",
    "        with tf.name_scope(\"User_Layer\"):\n",
    "            user_W1 = init_variable([self.shape[1], self.userLayer[0]], \"user_W1\") # self.shape[1] is number of items. For each user there are this number of items. That's the number of inputs for the neural network.\n",
    "            user_out = tf.matmul(user_input, user_W1)\n",
    "            for i in range(len(self.userLayer)-1):\n",
    "                W = init_variable([self.userLayer[i], self.userLayer[i+1]], \"user_W\"+str(i+2))\n",
    "                b = init_variable([self.userLayer[i+1]], \"user_b\"+str(i+2))\n",
    "                user_out = tf.nn.relu(tf.add(tf.matmul(user_out, W), b))\n",
    "\n",
    "        with tf.name_scope(\"Item_Layer\"):\n",
    "            item_W1 = init_variable([self.shape[0], self.itemLayer[0]], \"item_W1\")\n",
    "            item_out = tf.matmul(item_input, item_W1)\n",
    "            for i in range(0, len(self.itemLayer)-1):\n",
    "                W = init_variable([self.itemLayer[i], self.itemLayer[i+1]], \"item_W\"+str(i+2))\n",
    "                b = init_variable([self.itemLayer[i+1]], \"item_b\"+str(i+2))\n",
    "                item_out = tf.nn.relu(tf.add(tf.matmul(item_out, W), b))\n",
    "\n",
    "        norm_user_output = tf.sqrt(tf.reduce_sum(tf.square(user_out), axis=1))\n",
    "        norm_item_output = tf.sqrt(tf.reduce_sum(tf.square(item_out), axis=1))\n",
    "        self.y_ = tf.reduce_sum(tf.multiply(user_out, item_out), axis=1, keep_dims=False) / (norm_item_output * norm_user_output)\n",
    "        self.y_ = tf.maximum(1e-6, self.y_)\n",
    "\n",
    "    def add_loss(self):\n",
    "        regRate = self.rate / self.maxRate\n",
    "        losses = regRate * tf.log(self.y_) + (1 - regRate) * tf.log(1 - self.y_)\n",
    "        loss = -tf.reduce_sum(losses)\n",
    "        # regLoss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
    "        # self.loss = loss + self.reg * regLoss\n",
    "        self.loss = loss\n",
    "\n",
    "    def add_train_step(self):\n",
    "        '''\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        self.lr = tf.train.exponential_decay(self.lr, global_step,\n",
    "                                             self.decay_steps, self.decay_rate, staircase=True)\n",
    "        '''\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_step = optimizer.minimize(self.loss)\n",
    "\n",
    "    def init_sess(self):\n",
    "        self.config = tf.ConfigProto()\n",
    "        self.config.gpu_options.allow_growth = True\n",
    "        self.config.allow_soft_placement = True\n",
    "        self.sess = tf.Session(config=self.config)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "        if os.path.exists(self.checkPoint):\n",
    "            [os.remove(f) for f in os.listdir(self.checkPoint)]\n",
    "        else:\n",
    "            os.mkdir(self.checkPoint)\n",
    "\n",
    "    def run(self):\n",
    "        best_hr = -1\n",
    "        best_NDCG = -1\n",
    "        best_epoch = -1\n",
    "        print(\"Start Training!\")\n",
    "        for epoch in range(self.maxEpochs):\n",
    "            print(\"=\"*20+\"Epoch \", epoch, \"=\"*20)\n",
    "            self.run_epoch(self.sess)\n",
    "            print('='*50)\n",
    "            print(\"Start Evaluation!\")\n",
    "            hr, NDCG = self.evaluate(self.sess, self.topK)\n",
    "            print(\"Epoch \", epoch, \"HR: {}, NDCG: {}\".format(hr, NDCG))\n",
    "            if hr > best_hr or NDCG > best_NDCG:\n",
    "                best_hr = hr\n",
    "                best_NDCG = NDCG\n",
    "                best_epoch = epoch\n",
    "                self.saver.save(self.sess, self.checkPoint)\n",
    "            \"\"\"If it has been earlyStop number of epochs without the hr or NDCG being better, the algorithm stops.\"\"\"\n",
    "            if epoch - best_epoch > self.earlyStop:\n",
    "                print(\"Normal Early stop!\")\n",
    "                break\n",
    "            print(\"=\"*20+\"Epoch \", epoch, \"End\"+\"=\"*20)\n",
    "        print(\"Best hr: {}, NDCG: {}, At Epoch {}\".format(best_hr, best_NDCG, best_epoch))\n",
    "        print(\"Training complete!\")\n",
    "\n",
    "    def run_epoch(self, sess, verbose=10):\n",
    "        \"\"\"Getting training instances.\"\"\"\n",
    "        train_u, train_i, train_r = self.dataSet.getInstances(self.train, self.negNum)\n",
    "        \"\"\"Shuffle them.\"\"\"\n",
    "        train_len = len(train_u)\n",
    "        shuffled_idx = np.random.permutation(np.arange(train_len))\n",
    "        train_u = train_u[shuffled_idx]\n",
    "        train_i = train_i[shuffled_idx]\n",
    "        train_r = train_r[shuffled_idx]\n",
    "\n",
    "        num_batches = len(train_u) // self.batchSize + 1\n",
    "\n",
    "        losses = []\n",
    "        for i in range(num_batches):\n",
    "            min_idx = i * self.batchSize\n",
    "            max_idx = np.min([train_len, (i+1)*self.batchSize])\n",
    "            train_u_batch = train_u[min_idx: max_idx]\n",
    "            train_i_batch = train_i[min_idx: max_idx]\n",
    "            train_r_batch = train_r[min_idx: max_idx]\n",
    "\n",
    "            feed_dict = self.create_feed_dict(train_u_batch, train_i_batch, train_r_batch)\n",
    "            _, tmp_loss = sess.run([self.train_step, self.loss], feed_dict=feed_dict)\n",
    "            losses.append(tmp_loss)\n",
    "            if verbose and i % verbose == 0:\n",
    "                sys.stdout.write('\\r{} / {} : loss = {}'.format(\n",
    "                    i, num_batches, np.mean(losses[-verbose:])\n",
    "                ))\n",
    "                sys.stdout.flush()\n",
    "        loss = np.mean(losses)\n",
    "        print(\"\\nMean loss in this epoch is: {}\".format(loss))\n",
    "        return loss\n",
    "\n",
    "    def create_feed_dict(self, u, i, r=None, drop=None):\n",
    "        return {self.user: u,\n",
    "                self.item: i,\n",
    "                self.rate: r,\n",
    "                self.drop: drop}\n",
    "\n",
    "    def evaluate(self, sess, topK):\n",
    "        # Returns 1 if targetItem is a part of ranklist, 0 otherwise\n",
    "        def getHitRatio(ranklist, targetItem):\n",
    "            for item in ranklist:\n",
    "                if item == targetItem:\n",
    "                    return 1\n",
    "            return 0\n",
    "        \n",
    "        # Returns a higher number, closer to 1 the higher on the earlier in the ranklist the item appear (0 if it doen't appear)\n",
    "        def getNDCG(ranklist, targetItem):\n",
    "            for i in range(len(ranklist)):\n",
    "                item = ranklist[i]\n",
    "                if item == targetItem:\n",
    "                    return math.log(2) / math.log(i+2)\n",
    "            return 0\n",
    "\n",
    "\n",
    "        hr = []\n",
    "        NDCG = []\n",
    "        testUser = self.testNeg[0]\n",
    "        testItem = self.testNeg[1]\n",
    "        for i in range(len(testUser)):\n",
    "            target = testItem[i][0]\n",
    "            feed_dict = self.create_feed_dict(testUser[i], testItem[i])\n",
    "            predict = sess.run(self.y_, feed_dict=feed_dict)\n",
    "\n",
    "            item_score_dict = {}\n",
    "\n",
    "            for j in range(len(testItem[i])):\n",
    "                item = testItem[i][j]\n",
    "                item_score_dict[item] = predict[j]\n",
    "\n",
    "            ranklist = heapq.nlargest(topK, item_score_dict, key=item_score_dict.get)\n",
    "\n",
    "            tmp_hr = getHitRatio(ranklist, target)\n",
    "            tmp_NDCG = getNDCG(ranklist, target)\n",
    "            hr.append(tmp_hr)\n",
    "            NDCG.append(tmp_NDCG)\n",
    "        return np.mean(hr), np.mean(NDCG)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ml-1m data set...\n",
      "Loading Success!\n",
      "Data Info:\n",
      "\tUser Num: 6040\n",
      "\tItem Num: 3952\n",
      "\tData Size: 1000209\n",
      "WARNING:tensorflow:From /home/maria/.local/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py:132: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-4-da92138e46ae>:72: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/maria/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Start Training!\n",
      "====================Epoch  0 ====================\n",
      "2430 / 31068 : loss = 51.314758300781255"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Some code we need for running this in Jupyter Notebook\n",
    "    path = './data/ml-1m/ratings.dat' # I have no idea what this path should be, but this works\n",
    "    sequences = [str(record.seq) for record in SeqIO.parse(path, 'fasta')]\n",
    "    sys.argv = ['-f'] + sequences\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
